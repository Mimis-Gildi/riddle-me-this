---
categories: [reflections]
tags: [Aviation,Safety,Defensive Engineering,Automation]
excerpt: How hard would it be to apply more Defensive Engineering to aviation?
comments: true
classes: wide
---
= Air India 171: Defensive Engineering
:blog-url: https://mimis-gildi.github.io/riddle-me-this/
:blog-title: Creative Engineering at Scale
:blog-link: {blog-url}[{blog-title}]
:li-newsletter: https://www.linkedin.com/newsletters/behind-the-scenes-at-ase-7074840676026208257[Behind the Scenes at ASE,window=_blank,opts=nofollow]
:pub-li: see excerpt on LinkedIn by {li-newsletter}
:img-prefix: /riddle-me-this/assets/images
:cheese: https://en.wikipedia.org/wiki/Swiss_cheese_model[Swiss cheese model,window=_blank,opts=nofollow]
:cheese-fn: footnote:[{cheese} Wikipedia page]
:crashes: https://www.panish.law/aviation_accident_statistics.html[Aviation Crash Statistics | Updated 2025]
:crashes-fn: footnote:[{crashes} Panish Law]


.ASE Inc, DALL·E Hacker Dad vs. Cog Dad...
[#img-devs]
image::{img-prefix}/devs.png[Channel,800,450]

{nbsp}

Air India killed 260 -- onboard and on the ground. "Why did you cut off?" -- "I didn't do so."

From the preliminary report:

. Flaps set to 5 -- correct (spoilers extend accordingly). +
Takeoff configuration normal.
. V1 153 knots, 08:08:33 UTC.
. V2 155+ knots, 08:08:39 UTC -- rotation initiated.
. Wheels detach tarmac -- air mode automatically recognized.
. Max airspeed 180 knots, 08:08:42 UTC -- gear not retracted.
. Fuel Cutoff 1 (Captain side) 08:08:43 UTC -- ABNORMAL.
. Fuel Cutoff 2 (FO side) 08:08:44 UTC -- ABNORMAL.
. N1 Bleed Down begins 08:08:45--46 UTC -- immediate consequence.
. N2 Bleed Down 08:08:47 UTC -- full flameout.
. RAT Deploys 08:08:49 UTC -- terminal.

I ran 37 simulator scenarios on the 787-9 using the GEnx engine model (not the exact GEnx-1B used by Air India, but close enough).
*None* allowed recovery under 57 seconds from dual flameout at that gear-up point.

No recovery possible in the 40 seconds the pilot had.
Inertia too great. Engines too slow. No pilot can beat physics.

Rest in peace, innocent souls.

The critical fact: *the fuel cutoff toggles can only be moved by human action.*

This is now immutable history.

== General Context

Second absurd air crash I've researched this year.
I'm just an enthusiast glider pilot and an old systems engineer.
But this one's not about the bird. It's about us -- the humans.

In 1992, my PSU flight instructor told me: +
*"Kid, aviation rules are written in blood."*

Uptick in incidents is real -- nearly 6x 2022 baseline in my model. 
But don't panic: absolute numbers remain very low.
You're still safer in a plane than a car.
I'm compelled to watching this development nonetheless.

== Fuel Cutoff Switches

Speculation about electronic failure is unfounded.
Of course, I understand the human desire to protect the honor of the pilots.
And I also understand the engineering creed to safeguard life.

These switches are solid, with 50+ years of design history.
They don't fail casually -- sloped detents and dual-mode validation have stood the test of time.

Electrical detent states validations: 'Off', 'Detent low', 'Detent travel', 'Detent high', 'On' -- this isn't our house lights switch.

Pull, press through, hold, drop.
This requires intentional human action.

The odds of both switches failing this way are vanishingly low.
Sure, muscle memory can do it -- but only if commanded.

Reexamining the switch design leads to one conclusion: this was almost certainly a deliberate human act.

== Operating Laws

Modern airliners are packed with automation.
These birds operate in "Normal Law" unless a fault forces a switch to "Alternate Law".
In Normal Law, automation and envelope protections are active.
This flight remained in Normal Law.

_This detail matters for a point I'll make further down._

== Human Factors Psychology (HFP)

I've coached 21 major domain teams. Seventeen reached maturity; four did not.
What's the difference between a team that fails and one that succeeds?
**Just one thing: Basic Human Maturity.**

Here's my career-long discovery: meritocracy is self-healing.
When humans recognize their own bias, peer calibration happens.
That magic moment where a team self-corrects. No intervention required.
This collective state forms a self-healing culture -- often called defensive culture.

*Defensive Engineering* is the practice of designing to *safeguard against ourselves*.

Humans:

. Act from bias before logic;
. Don't auto-switch context;
. Are emotional first, rational second.

We've made a lot of progress in this area.
Consider this -- why do defibrillators have no OFF switch?
_Because we can't be trusted with one. And that's okay -- *if we consciously design for it*._

== CRM & Clean Cockpit

CRM is a direct product of human factors psychology and applied systems engineering.
Cockpits are not casual environments. What we do there has profound consequences.
Most of the time, however, we don't notice the *gloom it casts* over our souls.
That too is part of our nature -- we get acclimated.

CRM emerged alongside pair-based disciplines: firefighting, policing, combat arms.
_It's all about checking one another -- an *intrinsic validation channel*._
From a battle buddy kicking their partner's boots when the enemy is coming,
to a programmer reading their peer's code as they type.
Magic as old as time. We're paired creatures, not solitary ones.
Evolution even built in idle states to economize effort when paired.

Conversely, the biggest threat: *Power Gradient*. 
When one pair member dominates, the entire mechanism breaks to pieces. 
And we don't easily understand it. In engineering I take a lot of time to explain: *no peer, no pair programming*. 
Pair programming is impossible between non-equals!
No point pretending otherwise -- the benefit of the second mind is lost at onset.

"Why did you cut off?" -- "I didn't do so."
I will wait for the final report.
But even now, we can tell something was wrong in that cockpit.
Such scenarios are rarely a "root event" -- more often the final collapse in a long chain of preceding ones.{cheese-fn}

== Systemic Fix

Blaming human error is a dead end.
As mentioned earlier, aviation addressed this from the human side with CRM and Cockpit Power Gradients.
That, however, is just one face of the coin in fixing this tragedy of the human condition.
Is that really all we can do today?

No. We've had the means to *_prevent_* many such errors for years.
The science and engineering community has made notable strides.
But not everywhere.

So what really is *_the root challenge_* here?

**To err is human.**
Oh, but that's not a warning -- *that's a contract.*

Airbus and Boeing have danced around this: Boeing skewing human-dominant, Airbus leaning machine-dominant.
Neither solves the core problem.

But there's a third, beautifully additive path: +
{nbsp} **augmented operations**.

== Augmented Ops & MITL

Ukraine's GUR operation "Spiderweb" used AI-driven drones with real-time manual override.
Most people were confused -- automated or not.
The answer is not "yes, BUT" -- it's *Yes, AND*.
Aha -- that's what competence looks like.

This trickery is known as "Human-In-The-Loop" (HITL).
The machine does, but human can correct, and the machine continues to learn.

_But this magic works well in *both directions*!_

Enter Machine-in-the-Loop (MITL) systems.
Machines can observe, AND intervene -- to *stall a disaster* when necessary.

== MITL in Flight 171

Imagine this: a secondary system detects a fuel cutoff command during takeoff.
It delays the action briefly and prompts the crew:

*"In your current context this will kill everyone." +
"Is this really what you meant to do?"*

Culturally, we still struggle with this concept.
Every time I demo similar systems, I get objections from the room.
In this case, it'd sound like:

- Startle effect? Won't that make things worse for the pilots?
- How do you validate this? Can this even be safe?

Yes, it'll startle -- *that's the point!* +
And no -- *don't worry about validation*. +
Leave that to the team.

I teach these methodologies again and again.
Once the mindset shifts, teams get it.
They validate all logical branches.
And everyone ends up just a little bit safer.

=== MITL Simulations

In reality, people object to "uncommanded action."
Despite the fact that autopilot does that too.

For Jeju Air 2216, I built a hypothetical MITL layer.
It came down to just 37 high-confidence scenarios.
Gear up on final. Flaps off on takeoff. That kind of thing.

Right now, we handle this with noisy alerts.
Layered warnings. Contradictory signals. Masked failures.
Humans can't parse that in crisis.

What's different about MITL is *just-in-time action!*
And it can operate without touching any other systems.
Like auto-initiating a go-around in an obvious pickle.
Compare that to the forgotten TOGA button in Ahmedabad.

*_Flight 171 would've been saved._*

== What Would it Take?

I ran MLOps-based simulations across known human-error crashes:
Pilot disorientation. Takeoff config errors. Botched approaches.
In most of them, the cockpit was blaring -- adding to the chaos.

**My most optimistic estimate: 37 rules eliminate 67% of fatal human-error crashes.**
That's with a naive, first-gen MITL implementation.
Shooting for that number surprised me twice:
it didn’t take many rules -- and the payoff kept going.{crashes-fn}

Even crude scaffolding would help.
In one horrible crash, a two-second interjection would've saved everyone.
Literally everyone!

And there's even a better way.

Consider this: ~70--80% of fatal commercial crashes involve human error.
Of those, takeoff/approach issues -- checklist misses, config errors, cognitive overload -- make up 35--40%.

With an additive approach, we don't need to boil the ocean.
Go slow. Go carefully.
Validate each rule with the FAA -- one by one.
Begin with the no-brainers.

My conservative estimate:
Just 10--20 MITL rules could prevent ~45--55% of human-error-related accidents.

And we don't need much to make that happen.
The flight's already by wire.
There's room for a standalone system portable to most flying machines.

What's really required?

- Access to input streams (flight state, control actions);
- Simple rule-based validation + contextual interjection;
- Just 0.5--2 seconds of decision stalling, with the right cue.

That's it.

I can absolutely see this happening -- with the right team.

I'm skipping the integration hurdles -- and the compounding benefits to airline training maturity.

We let cars drive themselves.
Surely, planes can nudge once or twice -- when it really counts.

Fun fact: in the late 1970s, Cadillac added logic to prevent shutting off the engine or shifting into reverse at highway speed.
Yet in modern commercial jets, you can still *pull the ignition mid-takeoff* -- no questions asked -- knock yourself out.

Why are we holding aircraft with 200+ souls onboard to a lower standard than a 1970s Cadillac?


== Closing

Humans are guaranteed to make mistakes.
The cost of not acting: 260 lives, this time.

We've built the cloud. We've built Kubernetes. We've built war-grade AI drones. We've been to the moon.
And combat aircraft already *have* systems like this.

What gives?

Maybe we just need to *care*.
