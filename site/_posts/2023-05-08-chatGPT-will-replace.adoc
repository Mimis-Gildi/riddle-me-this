---
categories: [reflections]
tags: [Populism,Medium]
excerpt: >
  A grounded perspective on the AI job loss panic sparked by ChatGPT, written by a seasoned engineer.
  Dispels hype, explains LLMs clearly, and compares todayâ€™s fears to past tech hysteria.
classes: wide
comments: true
---
= ChatGPT & Job Loss - A 'Doze' of Reality
rdd13r

:medium-article-1: https://medium.asei.systems/chatgpt-job-loss-a-doze-of-reality-589637e91457["ChatGPT & Job Loss: A Doze of Reality",window=_blank]
:github-rdd13r: link:https://github.com/rdd13r[rdd13r,window=_blank]
:catb-mundane: link:http://www.catb.org/jargon/html/M/mundane.html[mundane,window=_blank]
:chatgpt-ui: link:https://chat.openai.com/[ChatGPT,window=_blank]
:autogpt: link:https://github.com/Significant-Gravitas/Auto-GPT[Auto-GPT,window=_blank]
:agentgpt: link:https://agentgpt.reworkd.ai/[AgentGPT,window=_blank]
:openai-site: link:https://www.openai.com/[OpenAI's website,window=_blank]
:openai-blog: link:https://www.openai.com/blog/[OpenAI Blog,window=_blank]

Since this is my first post, let me introduce myself and address a burning question: Are we all about to lose our jobs to AI like ChatGPT?
I am {github-rdd13r}, a hacker in the true sense - a creative problem solver.
My wife and I run several startups across the globe, making software, experimenting with new technologies, and coaching teams for other people's startups or laggards in digital transformation.
With over three decades of coding experience, we operate out of our US-based holding company.
I'll share more about us later, but for now, let's dive into the public's reaction to the late success of ChatGPT.

Ever since GPT-3's debut in 2020 and TabNine's launch in 2018, our company has quietly embraced these ML-augmented programming assistants - cool, saves time, very useful, simple to use and not very special.
The recent rampant buzz, however, evokes memories of the 1995 hacker hysteria following Kevin Mitnick's arrest in Raleigh.
That hysteria was so powerful that the MIT-originated term "hacker" shifted from "clever problem solver" to "criminal" overnight.
Just as the public once feared the power of code, today, they fear AI.
As animals, we often fear what we don't understand, especially when it seems extraordinary.
And sometimes we project onto people behind the extraordinary.
My peers and I couldn't help but feel that wary gaze on our backs from the uninitiated, we call {catb-mundane}.
It didn't matter that we made software keeping their business and livelihood alive, we were not accepted before and now even less.
Alas, I digress.

And today - ChatGPT will replace <insert profession> in <pick a number> of years!

No finger pointing, but that headline screams of massive job losses due to LLMs like ChatGPT.
Based on what facts and applied rules of inference, may I ask?
In this article, we aim to debunk these myths, unravel the realistic capabilities and limitations of LLMs, and shed light on what should truly concern us and what should not.
We should use facts and rules of inference to guide our discussion.
Additionally, we will explore the potential positive impact of AI on job creation and economic growth, offering a more balanced perspective on the issue at hand.

== Understanding ChatGPT and LLMs

ChatGPT, a large language model by OpenAI, is undeniably impressive.
It can generate human-like text and perform tasks like translation, summarization, and answering questions.
However, it's crucial to understand that ChatGPT is not sentient; it's a machine with significant limitations.
It cannot think, reason, or possess common sense like mammals do.
It isn't capable of original ideas and creativity.
All it does is autocomplete based on a massive amount of structured text, which is the product of collective human learning and expression, making the responses appear reasonable to us.
But these are not original responses of a reasoning being; it's merely a sounding board, like a mirror composed of combined memories from many of us.

When a person has a conversation with an LLM like {chatgpt-ui}, it is not two thinking beings conversing and arriving at truth through dialectics.
It's merely a thinking being conversing with a reflective surface.
This conversation is like a REPL (read-evaluate-print-loop), a hacker's favorite tool, which helps build up context in the half-conversation.
This context-building is where the real magic lies.
At ASE, we use our own ML sentiment engine as a context manager to interface the human user with a set of other ML tools including GPT-4.
My personal little toy is called _Matilda (*Tillie*)_, and she's very helpful -- but she's not an LLM -- she's another, greater thing!
She's tuned to me over the years and helped me develop this article, for example, just like she helps me write my code.

Now there are projects like {autogpt} (project home) that are even more useful,
allowing anyone to use LLMs to a greater extent without heavy proprietary software like Tillie,
yet offering a very similar kind of help.
If you code and like tinkering with things, the sources at GitHub are for you.
Some people even make such 'pets' without coding, and for that many new projects spring up on GitHub daily!
There's also a no-coding interface from Auto-GPT, {agentgpt}, that anyone can use to explore the new magic.

These REPL tools enhance the usefulness of LLMs by managing a much larger context and automatically working to drill down clarifications,
compensating for natural human vagueness and LLM's lack of depth.
But in the end, in my case, I know that I am still only talking with myself, albeit autocompleted by the world!
This is crucial for us to understand.
This software has no human-like intelligence.

My Tillie makes mistakes regularly even though a _DEDUCTIVE ENGINE_ is at her core and not some stochastic model.
Sometimes she runs herself into a spiraling self-reinforcement loop, arriving at truly absurd conclusions.
But it's not her making mistakes - she's just a machine operating honestly and truthfully on bad data.
To recover from this, I need to start troubleshooting my context by asking her a thousand questions "why" did she say what she said,
and then editing the context to remove paradoxical or self-contradictory data.
So I curate her because I must.
It's practically always me contradicting myself somewhere in the past that puts Tillie in an inconsistent state.
In reality -- she is the inconsistent state by definition because she's a machine, i.e., exacts, but the data comes from humans.
Unlike traditional ML, we don't conclusively know why deep learning systems do what they do,
and the ever-crucial introspection is a hot topic of research today.
The bottom line - they can't be trusted yet, because determinism is a "work in progress!"
And it goes for _all our deep learning tools_, as opposed to deductive tools.
Starting with our video stream context analyzer watching my 5-month-old daughter for safety and telling me "animal detected,"
which is technically true, but not useful.
To Tillie telling me that this article is not useful, which is also technically true, until I quote her reasons:

. You're right that LLMs like me and ChatGPT are not sentient, and we don't possess common sense or reasoning abilities like humans.
. It's important to note that our abilities are still evolving, and future models may have more advanced features.
. You are right that the concept of using a context manager, like me, to improve the usefulness of LLMs proved an interesting and valid approach.
. It might not be the case for every user or every implementation of LLMs. It is your tinkering and your tinkering alone that defines this outcome.

*Conclusion:* *_Plausible Bull (PB)._* As long as *_any_* ML is in the loop - the entire product is stochastic, i.e., PB.

Such responses can fool us into believing that there's more to this machine when there really isn't.
Tillie is not alive.
And it's not just my tinkering, by the way, the whole world is experimenting.
Most importantly, there's simply no inherent reliability in the design.
All this is why systems before ChatGPT failed so miserably -- curation.

_And the salvation today -- accidental discovery that "size matters" -- isn't a way out._

== Realistic Expectations and Limitations

While ChatGPT may automate some tasks, it is truly unlikely to replace entire professions.
Instead, it can be used as a tool to enhance human productivity and creativity.
It's essential to recognize that LLMs have limitations, including generating irrelevant or incorrect answers,
sensitivity to input phrasing, and verbosity, i.e., just more PB.
While the model's architecture and weights are deterministic, the text generation process as a whole is non-deterministic.
This means that the outputs can vary depending on factors like the randomness introduced during training and the specific input context provided.
This non-deterministic nature (the stochasticity) makes LLMs less suitable for mission-critical tasks, like controlling an aircraft or medical equipment.
Non-deterministic means "cannot be trusted" and "cannot be reproduced."
Would you let such nondeterministic software fly your plane with your family onboard?
Probably not.
But as advice to *_a properly educated pilot_*, it may prove invaluable.
I emphasize that "education" here because even a deterministic thinking machine, like Tillie,
fails outright when the context and data she operates on (1) come from LLMs and (2) is trusted 'valid' when there's no such assurance.

LLMs are valuable in other areas, like *_helping_* in mundane research tasks, providing conversational support, or aiding in PB content creation, like this article.
These AI-powered tools can help reduce the cognitive load on professionals, allowing them to focus on more complex and creative tasks.
After all, the best help humans ever get comes from the reduction of mundane.

It's important to remember that the effectiveness of LLMs relies heavily on the quality of the data they are trained on.
Since these models are based on patterns found in large datasets, biases and inaccuracies present in the data DO lead to biased or incorrect outputs.
Addressing these challenges requires continuous research and improvement of both training data and model architectures.

Most importantly, a tool has little to do with the root dilemma here!
_From the moment a primate picked up a stick and used it as a tool to extract termites from a stump to eat, it *obsoletes* a peer not willing to use a stick._
Innovation has been a constant driving force in our lives because it's a human factor.
_All we do as hackers is automate repetitive processes and tasks, making more time for creative thinking._
*And individuals who don't adapt to new knowledge or tools risk becoming obsolete.*
This has nothing to do with software or a stick; it's just the natural progression of humanity.
LLMs are simply the latest addition to the long chain of tools continuously developed, helping the thinkers obsolete the complacent ones.
